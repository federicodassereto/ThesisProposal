\section{Goals, Methodology and Preliminaries}\label{goals}
In this section, we start with an highlight of the macro goals of the thesis, then we expand the content of Section \ref{motivations} by detailing our specific goals and the feasibility of each of them. Finally, we summarize the work done in the first year concerning the proposed goals.

\subsection{Goals}\label{sub_goals}
The high-level goals of this thesis are:
\begin{itemize}
    \item To show the relevant role that embeddings can play in searching tasks;
    \item To show that embeddings can be used directly to answer queries instead of being an orthogonal part of the answering process.
\end{itemize}

The overall goal of this research proposal is the study of the role of embeddings in data augmentation, from tables augmentation on data lakes to an embedding algorithm for knowledge bases, passing through the integration of embeddings in existing tables augmentation framework exploiting knowledge bases. We organize the work into four main objectives, each leading to specific results. 
The aim of Objective 1 (indexing data lakes to augment) is to analyze the state of the art on (i) open data search, and (ii) sketching and approximation of information-theoretic measures, and to define an index for tables augmentation in data lakes based on information-theoretic measures. 
The aim of Objective 2 (augmenting via knowledge bases) is to analyze the state of the art approaches in knowledge bases representation and augmentation, along with their role in augmenting tables and integrating of embeddings in such approaches. 
Objective 3 (embedding knowledge bases) aims at proposing an embedding algorithm for knowledge bases, by creating a mathematical structure that encapsulates relevant features for catching the bast augmentation possible, and an evaluation of the proposed algorithm. 
Finally, Objective 4 (stay up-to-date on embeddings) is an objective which is equally distributed over the three years, in order to stay up-to-date with the newest embedding technologies and algorithms. It is worth to be over the three years since it is a very dynamic topic, and the knowledge gained in recent years on the subject would not be lost.

\bigbreak

\noindent\textbf{Objective 1: Indexing data lakes to augment.}
\begin{enumerate}
    \item \textit{Analysis of the state of the art approaches} related to (i) open data searching and indexing, and (ii) information-theoretic measures approximation and sketching. There exist many frameworks aiming at searching on open data, for tasks such as joinability and unionability, implementing different techniques. Information-theoretic measures are very useful and, for large datasets, can require a lot of time to be computed. We plan to study how to approximate such measures as well as sketching them on a subset of the available data.
    \item \textit{Definition of an index for tables augmentation in data lakes} based on information-theoretic measures, taking advantage of existing indexing structures (e.g., inverted index) by storing the information about a table and its columns, along with information theory measures regarding columns. In such a way, exploiting the fast search offered by the index structures, it would be possible to retrieve joining columns. The key idea is that we will exploit information-theoretic measures to simulate the behavior of functional dependencies. 
\end{enumerate}


\noindent\textbf{Objective 2: Augmenting via knowledge bases.}
\begin{enumerate}
    \item \textit{Analysis of the state of the art approaches} for knowledge bases construction, representation and augmentation, along with their role in augmenting tables and integrating of embeddings in such approaches. Since the variety of knowledge bases available, often built available as knowledge graphs, this analysis will be deeply and sharply isolate the fundamental features that a KB is required to have.
    \item \textit{Definition of a framework that exploits knowledge bases and possibility of including embedding to augment}, by understanding how embeddings algorithms works on knowledge bases and find the best possible representation to maximize the augmentation. Knowledge bases semantics can be very useful in extending tables, by adding not only features to maximize the augmentation but also to add new relevant tuples. Once the augmentation through embeddings will be completed, a comparison between with and without embedding approaches will be conducted, in order to highlight the key role of embeddings.
\end{enumerate}

\noindent\textbf{Objective 3: Embedding knowledge bases.}
\begin{enumerate}
    \item \textit{Definition of an embedding algorithm for knowledge bases}, by representing in the best way the isolated relevant and general properties that KBs should have for being suitably embedded, along with an extensive evaluation on its efficiency compared to existing embedding techniques. To the best of our knowledge, in the state of the art of embedding algorithms, no algorithm exists that exploits the semantics of its entities, rather they exploit the structural properties of the knowledge base to project it onto a new semantic space. This objective will be founded on the results obtained in Objectives 1 and 2, in order to understand if embeddings effectively play a role. Since this objective is the farthest from the time of writing, not many details are reported.
\end{enumerate}

\noindent\textbf{Objective 4: Stay up-to-date on embedding techniques.}
\begin{enumerate}
    \item \textit{Analysis of the state of the art approaches}, of continuously published embeddings works. Many embeddings approaches are yearly proposed, in a variety of formats: from completely new approaches to slightly changes on existing ones, from pre-trained embedding on large structures to new benchmarks. This objective includes the tracking of theoretical new ideas and implementations, in order to able to integrate new embeddings ideas in this work.
\end{enumerate}


\subsection{Methodology}\label{sub_methodology}
In the following, we describe the methodology we envision for achieving each objective described in Section \ref{sub_goals}.

\noindent\textbf{Objective 1: Indexing data lakes to augment.}
\begin{enumerate}
    \item \textit{Analysis of the state of the art approaches} related to (i) open data searching and indexing, and (ii) information-theoretic measures approximation and sketching. We will review the most relevant solutions to open data problems, like table joinability and unionability, as well as the relevant results obtained in approximating and sketching information-theoretic measures. To this aim, we will take into account both the scientific literature related to data management and to data analysis and learning.
    \item \textit{Definition of an index for tables augmentation in data lakes} based on information-theoretic measures. We plan to propose and index for automatic table augmentation in data lakes. In particular, given a table $T$, a join column $t_j$ of $T$, a target column $t_y$ of $T$ and a data lake $D$ composed of $n$ tables ($D_1,...,D_n$), the index will return a ranking of tables in $D$, joining on $t_j$ that provides the best augmentation for $T$, i.e., mostly improves the performances of a machine learning model that has $t_y$ as target.
\end{enumerate}

\noindent\textbf{Objective 2: Augmenting via knowledge bases.}
\begin{enumerate}
    \item \textit{Analysis of the state of the art approaches} for knowledge bases construction, representations and augmentations. We will conduct a deep and complete analysis of the state of the art approaches and algorithms to manipulate and represent KBs, as well as embedding algorithms for KBs. The main distinction in the embedding approaches will be among the ones with a geometric interpretation, i.e., transitional operations between vectors and the ones based on matrix factorization and co-occurrences, i.e., the family of embedding closer to word embeddings.
    \item \textit{Definition of a framework that exploits knowledge bases and possibility of including embedding to augment and Comparison}. In the case of semantic data lakes, i.e., data lakes with known (or derivable) mappings on KGs, embeddings can be exploited to retrieve relations between entities in the KG semantic space (for example through a nearest neighbors search or even more complex ways) and produce the augmentation. Once the framework is designed and implemented, an evaluation of the two scenarios (augmenting via KGs only and KGs embedding) will be conducted to show the advantages of using embeddings. In such a situation, an augmentation using only KGs will be taken as baseline; it could work, in its simpler version, with a mapping from tables values to entities in the KG on which a basic join operation can be performed.
\end{enumerate}

\noindent\textbf{Objective 3: Embedding knowledge bases.}
\begin{enumerate}
    \item \textit{Definition of an embedding algorithm for knowledge bases}. Once Objective 2 is completed, we will have a picture of what features of existing KGs embedding have a positive impact on the augmentation and which one could be discarded on modified. To this aim, extensive knowledge of embedding algorithms will result in the definition of ad-hoc algorithms to embed KGs to provide table augmentation. The key concept for such an objective is that we do not want to propose a complete novel algorithm for embedding KBs, rather we plan to isolate and represent, in an appropriate way, the most important features of the KB to improve existing algorithms for KBs embeddings. In a sense, we want to integrate the semantic lying in the KB in the embedding process of existing algorithms, that up to now only consider structural properties. At such a point, a comparison between all of the approaches will be done: tables augmentation in data lakes only, table augmentation via KGs, table augmentation via KGs embeddings, and table augmentation via our KGs embedding algorithm.
\end{enumerate}

\noindent\textbf{Objective 4: Stay up-to-date on embedding techniques.}
\begin{enumerate}
    \item \textit{Analysis of the state of the art approaches}, of continuously published embeddings works. In order to review and keep trace of the most relevant embedding approaches, the surveyed techniques we will then be organized into a taxonomy, according to their hypothesis, usage and domain of application.
\end{enumerate}



\subsection{Preliminaries}\label{sub_preliminaries}
In this section we will present the activities done and the results achieved in the first year.

\begin{figure}[t]\label{index_sketch}
    \includegraphics[scale=0.4]{figures/index_sketch.png}
    \caption{Sketch of entropy-based index for open data. When a value $v$ is present in column $j$ of table $i$, in position $(v,i)$ it is stored the entropy of the column of $j$ of table $i$.}
\end{figure}

\begin{enumerate}
    \item We have analyzed the state of the art approaches related to (i) open data searching and indexing, (ii) information-theoretic measures sketching and approximations and (iii) functional dependencies discovery. We found out that existing searching frameworks on open data only cares about finding the best, or the best \textit{top-k} set, of \textit{joining} tables or columns. This has many drawbacks, first of which the fact that the best join does not say anything about the best augmentation. For instance, the best join that a table can have is with itself, meaning a void augmentation. 
    \item We also noticed that there are information-theoretic measures that are particularly suitable in representing, or at least approximating, the behavior of functional dependencies. In particular, the conditional entropy $H(Y|X) = H(X)-I(X,Y)$, where $H(X)$ in the Shannon entropy of variable $X$ and $I(X,Y)$ is the mutual information between variables $X$ and $Y$, has lower bound 0 and upper bound $H(X)$. These two cases coincides with full functional dependency\footnote{Note that, despite the differentiation that we made in Section \ref{related} about functional dependency, up to now the interpretation is the same on classical database FDs and descriptive FDs.} when $H(Y|X) = 0$ (meaning that the value of $Y$ is totally determined by $X$) and complete independence, when $H(Y|X) = H(Y)$ (meaning that the knowledge of variable $X$ has no impact on the knowledge of $Y$). This observation, mixed with the intuition that if a functional dependency (in its descriptive interpretation) between a set of attributes and a specific target holds these attributes are interesting for the augmentation, means that discovering FDs that minimize the conditional entropy\footnote{Actually, our intuition derived by the fraction of information, i.e., a normalized variant of the mutual information defined as $\frac{I(X;Y)}{H(X)}$. However, it is possible to show that minimizing the conditional entropy is equal to maximize the fraction of information.} produce the best augmentation. However, the approach of identifying a target attribute and discover FDs in such a way requires the materialization of the join, that, as discussed in Section \ref{related}, is unfeasible on data lakes.
    \item We also analyzed the role of Functional Dependencies (FDs). FDs express relationships between attributes of a database relation. An FD $X$ $\rightarrow$ $Y$ states that the values of attribute set X \textit{uniquely} determine the values of attribute $Y$. Despite their rich semantic information, functional dependencies are extremely hard to compute, as they belong to the W[2] complexity class \cite{blasius2017parameterized}. Particularly interesting are the non-trivial (FDs that cannot be derived from others) and minimal functional dependencies. Many works have been proposed to mine functional dependencies from tables: lattice-based approaches such as TANE \cite{huhtala1999tane} and DFD \cite{abedjan2014dfd} that performs search and pruning on the lattice structure; row-based methods derive candidate FDs from two attribute subsets, namely agree sets and difference-sets, which are built by comparing the values of attributes for all possible combinations of tuples pairs, such as DepMiner \cite{lopes2000efficient} and FastFD \cite{wyss2001fastfds}; incremental approaches exploiting the concepts of tuple partitions and monotonicity of FDs to avoid the re-scanning of the database \cite{wang2001incremental}. Of course, all of these approaches fail to scale to very large tables both in terms of time and memory usage. 

    We analyzed many of above-mentioned FDs discovery algorithms, both exact and approximate, implemented in a popular framework named Metanome \cite{papenbrock2015data}. We found out, as expected, that no existing algorithm for FDs discovery is scalable to data lakes of thousands of tables and many Gigabytes of space. All of the experiments we run on single tables with thousands of rows and 30-40 attributes result in memory exceeding error or timeout. In sight of this, we chose to not follow the direction of pure FDs discovery, but to limit their usage to theoretical studies, such as upper and lower bounds studies and pruning techniques to improve our information-theoretic based index.
    
    The idea of inferring FDs from data has attracted a different view of such concept: the point of view change from \textit{uniquely implies} (roughly speaking, each left side of FDs is key with respect to the right side) to \textit{better identify} (each left side of FDs is a "substitute" of the right side). These kinds of FDs quantifies how much the left side approximates the right side; it very useful in the situation in which correlations discovery and features elimination is needed. The mining of such an idea of FDs has been proposed by using information-theoretic measures like the reliable fraction of information \cite{mandros2017discovering} and the smoothed mutual information \cite{pennerath2020discovering}. The (partial) drawbacks of these last approaches are that (i) they require to know which variable the user wants to "describe", i.e., the right side of the functional dependency and (ii) the right side must be composed of a single attribute, while the left side is generally made by several attributes. Furthermore, these approaches does not scale well with large tables with many attributes, while are able to manage table with many rows.
    
    \item To overcome the issue of the join materialization, we decided to exploit the idea of an inverted index, in which the information of each table is stored, along with each column entropy and the column-table association. The peculiarity of this index is that it looks like a sparse matrix, since all the unique values in all the tables are indexed, and the entropy information for each column is stored only when i-th values belongs to j-th column. Figure \ref{index_sketch} shows a sketch of the designed index.
    \item We are currently working on the implementation of such an index, along with the research for understanding if mixing up the entropies is enough to simulate the conditional entropy behavior or a bit more elaborated metrics is required.
\end{enumerate}

