\section{State of the Art}\label{related}
% In this section, we discuss related work that we consider relevant for the proposed research project. In order to facilitate reading, the discussion has been organized into five main sections, corresponding to the main concepts introduced in Section \ref{motivations} and \ref{reference}. Each of the five sections is in turn divided into sub-parts to further facilitate reading. The first one discusses embeddings approaches and their relevance to the proposal, the second introduces Open Data and their common operations, the third discuss knowledge graphs, the fourth provides an overview of existing augmentation approaches and finally the fifth reviews functional dependencies discovery approaches.
In this section, we discuss related work that we consider relevant for the proposed research project. In order to facilitate reading, the discussion has been organized into three main sections. Each of the three sections is in turn divided into sub-parts to further facilitate reading. The first one discusses Open Data and their common operations, as well as existing augmentation approaches, the second one introduces embeddings approaches and their relevance to the proposal, finally the third discusses knowledge graphs.

\subsection{Open Data Integration}
The increased spreading of Open Data has been possible thanks to the fact that many organizations publish their data following the Open Data principles \cite{bizer2009linked}. As discussed in Section \ref{motivations}, these data are continuously produced and stored; their dynamic nature makes it impossible to apply previous management techniques, such as the creation of a global schema \cite{10.1145/27633.27634} and to keep track of join paths known or mined from the data\cite{fagin2009clio,deng2017data}. All of the above-mentioned techniques requires tables to have schema information or meaningful attribute names, as well as to manage pre-computed join-paths. Both these practices are impracticable for an open platform at Internet scale, since the number of tables can easily reach millions or more. The most relevant task while dealing with Open Data is the discovery part, which includes searching for tables containing specific value(s) based on keywords \cite{brickley2019google} or containment. Reconnecting to what we discussed in Section \ref{motivations}, we further identify two macro-areas in Open Data discovery, namely Join Search and Union Search.
\bigbreak

\textbf{Join Search.} Given a query table $T_q(A_1,A_2,...)$ with join attribute $A_j$, a joinable table is a relation $T(B_1, B_2,...)$ such that $T$ has at least one attribute $B_i$ that is equi-joinable with $A_j$. To be more precise, the values in the two attributes must have significant overlap. The problem has been largely posed as set similarity search, where the values of the attributes are sets and a similarity function determines the relatedness. A frequent similarity function is the Jaccard Index, which suffers to the problem of unfairly advantages small domains \cite{zhu2016lsh}. Many solutions have been proposed exploiting this idea, but they all work under the assumption of having tables with average sets size ranging from few columns to a maximum of few hundreds. This is not the case of open data lakes, where there are hundreds of thousands of tables possibly with millions of rows. To solve this issue,  LSH Ensemble \cite{zhu2016lsh}, an index based on locality-sensitive hashing \cite{gionis1999similarity} and MinHash \cite{indyk1998approximate} that uses containment, poses the problem as a domain search, where each attribute of a table is a domain. LSH Ensemble requires on a threshold value, making it an approximate algorithm for joining tables discovery. The family of approximate algorithms are scalable in performance, however, they tend to suffer from false positive and negative errors, especially when the distribution of set sizes is skewed (as frequent in data lakes). Furthermore, using a threshold may confuse users, who have no knowledge of which data exists in the lake and therefore do not know how to set a reasonable threshold that will retrieve some, but not too many answers. A possible alternative to threshold search is to retrieve the top-k tables with higher containment. Another effective algorithm for join search is JOSIE \cite{zhu2019josie}, a scalable exact top-k overlap set similarity search algorithm. JOSIE minimizes the cost of set reads and exploits an inverted index to find the top-k sets. Due to its inverted index structure and the prefix filter, a fast trick to solve the threshold version of the set-similarity search problem \cite{chaudhuri2006primitive}, JOSIE outperforms its approximate counterparts for small k.
\bigbreak

\textbf{Union Search.} Given a query table $T_q(A_1,A_2,...,A_n)$ with $n$ attributes, a unionable table is a table $T(B_1, B_2,...,B_k)$ such that $T$ has at least one attribute $B_i$ that is unionable with some attribute $A_j$ from the query table. Unionability means that, given attribute $A_j$ and its domain (set of values), and attribute $B_i$ and its domain, it is likely to exist a third domain $D$ from which both $A_j$ and $B_i$ are sampled. An approach to finding unionable tables is through schema matching, where the problem is to match the attributes of two or more tables (or schemas) \cite{he2003statistical,rahm2011towards}. Two tables that match on $i$ attributes can presumably be unioned on those attributes. Matching is done largely heuristically using similarity functions over schema (attribute names) and sometimes values (for example, using a set similarity measure) or value distributions \cite{kang2003schema}. Although scalable schema matching and ontology alignment have been studied extensively, the best solutions are drawn when considering the unionability as a search problem. In particular, in \cite{nargesian2018table} they find the k tables that have the highest likelihood of being unionable with a search table $T$ on some subset of attributes. In few words, they determine if a table $S$ that can be unioned with $T$ on $c$ attributes is more or less unionable than a table $R$ that can only be unioned on $d < c$ attributes.
\bigbreak

%\subsection{Data Integration \& Augmentation}
\textbf{Data Integration \& Augmentation.} The natural consequence of joining or unioning tables is an augmented dataset, i.e., a dataset with more attributes or samples than the original one. The tricky point is to produce augmentations that are aware of all the content of the tables and not only on the join/union attributes. In other words, the augmentation should not only manifest in larger or taller tables but in more meaningful tables contents. There has been extensive prior work on data mining, data augmentation, knowledge discovery, and feature selection, but only a few approaches to automatic augmentation have been proposed. Kumar et al. \cite{kumar2016join} proposed a set of rules to determine if avoiding to perform a join would be safe in a relational context, while in \cite{shah2017key} the rules of \cite{kumar2016join} are applied to high-capacity classifiers to test their validity. Other approaches to augmentation are devoted return a set of related tables, such as \cite{bogatu2020dataset}, where a relatedness search in data lakes is performed, and \cite{fernandez2018aurum} that automatically identify joins between tables representing similar entities. While these systems help users to discover new data and explore relationships between datasets, they do not automatically determine whether or not such new information
is useful for a predictive model. To overcome this lack, recently a framework called ARDA \cite{chepurko2020arda} has been proposed. ARDA works as a two steps algorithm, firstly it searches for join and then prunes out irrelevant features with features selection algorithms. ARDA materializes the joins between the input table and the tables in data lakes, making it difficult to ensure scalability (in fact, their experiments are run on data lakes of less than a hundred tables).

\subsection{Embeddings}
Embeddings allow systems to capture non-geometric data in a mathematical structure, useful for easier comparison of data. A lot of attention has been devoted to word embeddings, i.e., embeddings of documents in which every word is represented by a vector. 

Word embeddings have been proposed in an NLP context thanks to their ability to capture semantic relations among words in a text. 
Word2Vec \cite{Mikolov2013EfficientEO} is an example of a pre-trained embedding that embeds into Euclidean Space. 
Later, other embedding techniques like GloVe \cite{pennington2014glove}, BERT \cite{devlin2018bert} and ELMo \cite{peters2018deep} have been proposed, only to mention the most famous. 
All these algorithms belong to the family of Euclidean embeddings, i.e., project texts onto a Euclidean space. 
Although these methods are very effective on texts, they fail to accurately capture different kind of sources such as graphical structures \cite{NIPS2017_7213}. 

In contrast, hyperbolic embeddings are particularly suitable to embed hierarchical data. 
As described in \cite{krioukov2010hyperbolic}, hierarchical structures show a hidden hyperbolic geometry. 
On such observation, many hyperbolic embeddings have been proposed: the Poincar\'e Disk Model \cite{NIPS2017_7213}, the Lorentz Model \cite{DBLP:journals/corr/abs-1806-03417} and a convex entailment cones approach \cite{ganea2018hyperbolic}. 
It is worth to notice that Poincar\'e and Lorentz models produce the same projection in different spaces; the only difference is in the way they are computed, since the Lorentz model leads to more stable computations. 
Finally, a mixed approach was proposed in \cite{gu2018learning} to take advantage of the properties of both hyperbolic and Euclidean spaces. 

Along with the rise of word embeddings, the need of quality indicators for these structures was raised too. 
Initially, no common way of assessing the quality of embeddings was developed. 
Each embedding algorithm was evaluated in a task-dependent way, which means that if the embedding performs well on a particular task it is considered "good". 
Recently, the concept of distortion was introduced in \cite{sala2018representation} and slightly modified in \cite{dassereto2019evaluating}. 
To the best of our knowledge, \cite{dassereto2019evaluating} is the first attempt to evaluate the use of embeddings in a geographical context. 
The distortion essentially measures how well the distances in original metric space are preserved in the embedding. Thus, it can be seen as a task-independent evaluation. In hyperbolic embeddings, all the links between entities in the structure are treated equally, but this could not be always the case; in such a situation, Knowledge Graphs Embeddings (KGE) can be applied. 

% Few approaches have been proposed to integrate embeddings in a geospatial context, for example in \cite{di2020sherloc} they are used to search semantic neighbors of geographical terms, while in \cite{mai2019relaxing} they are exploited to relax unanswerable geographic questions. 
Finally, hybrid approaches mixing hyperbolic and Euclidean embeddings have been proposed, based on the observation that in a text there are many asymmetric word relations. The main approaches in this direction are an adaption of GloVe in a Cartesian product of spaces \cite{tifrea2018poincare} and a learning model in a spherical space \cite{meng2019spherical}.  

As mentioned in Section \ref{motivations}, a preliminary approach exploiting embeddings for data integration tasks have been proposed \cite{cappuzzo2020creating}. In such an approach, tables are represented as graphs, by adding nodes representing columns and rows identifiers and then embedding in Euclidean spaces following the framework of the random walk. The drawback of such an approach is that is assumed to be known context from which each table comes from, making it unsuitable for our scenario.
%{\color{red}{AGGIUNGI Keynote PODS qua o sopra in motivations(meglio)}}

\subsection{Knowledge Graphs}
A knowledge graph is a set of triples, in the form \textbf{$\langle$ s,p,o $\rangle$}, where \textbf{s} and \textbf{o}, subject and object respectively, are entities and \textbf{p} is the predicate between \textbf{s} and \textbf{o}. KGs are traditionally stored as RDF (Resource Description Framework) \cite{colazzo2014rdf} triples. Since a rigorous definition of KGs does not exists, according to \cite{paulheim2017knowledge} any RDF graph can be considered a KG, as long as they (i) mainly describes real-world entities and their interrelations, organized in a graph; (ii) defines possible classes and relations of entities in a schema; (iii) allows for potentially interrelating arbitrary entities with each other and (iv) covers various topical domains. This general definition allows us to consider KGs structures like DBpedia \cite{lehmann2015dbpedia} and its ontology, a knowledge base extracted from Wikipedia and YAGO, \cite{suchanek2007yago} which is linked as well as the DBpedia ontology. A complete survey on how to build, model and query KGs can be found in \cite{ji2020survey}.
Recently, the idea of embedding KGs emerged, to include them in machine learning tasks such as links prediction \cite{rossi2020knowledge} and question answering \cite{huang2019knowledge}. A very effective algorithm called TransE was proposed in \cite{bordes2013translating}, in which embeddings are learned in a translational way; this work generated many variants such as TransH \cite{wang2014knowledge} and TransR \cite{lin2015learning}. A thorough survey on embedding techniques for KGs can be found in \cite{goyal2018graph}.



% \subsection{Functional Dependencies}\label{fds}
% Functional dependencies (FDs) express relationships between attributes of a database relation. An FD $X$ $\rightarrow$ $Y$ states that the values of attribute set X \textit{uniquely} determine the values of attribute $Y$. Despite their rich semantic information, functional dependencies are extremely hard to compute, as they belong to the W[2] complexity class \cite{blasius2017parameterized}. Particularly interesting are the non-trivial (FDs that cannot be derived from others) and minimal functional dependencies. Many works have been proposed to mine functional dependencies from tables: lattice-based approaches such as TANE \cite{huhtala1999tane} and DFD \cite{abedjan2014dfd} that performs search and pruning on the lattice structure; row-based methods derive candidate FDs from two attribute subsets, namely agree sets and difference-sets, which are built by comparing the values of attributes for all possible combinations of tuples pairs, such as DepMiner \cite{lopes2000efficient} and FastFD \cite{wyss2001fastfds}; incremental approaches exploiting the concepts of tuple partitions and monotonicity of FDs to avoid the re-scanning of the database \cite{wang2001incremental}. Of course, all of these approaches fail to scale to very large tables both in terms of time and memory usage. The idea of inferring FDs from data has attracted a different view of such concept: the point of view change from \textit{uniquely implies} (roughly speaking, each left side of FDs is key with respect to the right side) to \textit{better identify} (each left side of FDs is a "substitute" of the right side). These kinds of FDs quantifies how much the left side approximates the right side; it very useful in the situation in which correlations discovery and features elimination is needed. The mining of such an idea of FDs has been proposed by using information-theoretic measures like the reliable fraction of information \cite{mandros2017discovering} and the smoothed mutual information \cite{pennerath2020discovering}. The (partial) drawback of these last approaches is that they require to know which variable the user wants to "describe", i.e., the right side of the functional dependency. Furthermore, the right side must be composed of a single attribute, while the left side is generally made by several attributes.

