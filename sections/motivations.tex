\section{Motivation and Description}\label{motivations}

Since the rise of the World Wide Web, we have experienced an exponential growth in publicly available data. Data has become a crucial element in everyday life and many devices continuously produce and store them in a forest of different formats. This exponential growth has posed many challenges to scientists (even going so far as to create the figure of the \textit{data scientist}) in every step in which data are traditionally used: from data collection to integration, from processing to visualization. The impact of \textit{Big Data} has been summarized in \cite{mcafee2012big}, where the key properties are \textbf{V}olume, \textbf{V}elocity, \textbf{V}ariety, \textbf{V}eracity and \textbf{V}alue. Following the path traced by Tim Berners-Lee et al. in \cite{bizer2009linked} regarding Open Data publication and maintenance, many organizations are publishing data, augmenting tremendously the \textbf{V}olume and the \textbf{V}alue of such data. On the other hand, the explosion of sources providing data increases their \textbf{V}ariety, requiring new techniques to integrate data.

In data science, it is increasingly the case that the main challenge is not in integrating known data, rather it is in finding the right data to solve a given data science problem. Today, data is a mass (uncountable) like dust, and data surrounds us like dust, even lovely structured data. Data is so cheap and easy to obtain that it is no longer important to always get the integration right and integrations are not static things (\textbf{V}elocity component). Data integration research has embraced and prospered by using approximation and machine learning. 

A data scientist who is trying to solve machine learning tasks, frequently found herself in a situation in which there are not enough data to build a good model. Given the amount of data previously discussed, a framework which enables her to find automatically new features or samples to improve the model would be very useful. Broadly, two main classes of augmentation can be distinguished: \textit{Horizontal Augmentation}, in which new significant features are added to the dataset, and \textit{Vertical Augmentation}, in which new samples (tuples) are added to the dataset. Ideally, the two classes can be seen as the search for joinable (Horizontal) or unionable (Vertical) tables in the data lakes situation, and Knowledge Bases (KB) completion or extensions (Vertical).

%{\color{red}Piu schematico su problemi, definizione e caratteristiche.}
The uncontrolled nature of data manifests in large repositories of data (\textit{data lakes}), in which both structured and unstructured data are stored. In data lakes, it is common the situation in which there is a lack of schemas, making traditional database approaches to integrating or querying data difficult to pursuit or even infeasible. In the absence of a data schema, an effective metadata system becomes essential to make data queryable and thus prevent the lake from turning into a data swamp, i.e., an unusable data lake \cite{walker2015personal,hai2016constance}. Nevertheless, it is frequent the case in which metadata are not provided. To overcome this issue, approaches mining metadata from tables have been proposed \cite{arocena2015ibench,suriarachchi2016crossing}.
Along with the uncertainty regarding the quality of the data, the amount of such data makes it infeasible also the traditional human-in-the-loop framework, since hand-labeling or manual rating of very large amounts of data is extremely expensive. It is easy to see that searching in data lakes is a complicated task, both in terms of time and methodology. 

At a processing level, the main difficulties arisen by the multiple sources and possibly unstructured data are given by the interpretation, i.e., the semantic layer, and the absence of a common schema. In particular, the absence of a schema makes it unfeasible to exploit traditional data management operations. To extract semantics from unstructured data, in the last years the concept of \textit{embedding} has been proposed. Embeddings are predominantly used in learning representations of texts \cite{Mikolov2013EfficientEO} and graphs \cite{NIPS2017_7213}. In the last year, an approach exploiting embeddings to solve data integration tasks \cite{cappuzzo2020creating} has been published. This work can be seen as a forerunner for the integration of embeddings in data management tasks, even if it works on tables from the same context. A key contribution to the integration of embeddings by Martin Grohe in \cite{grohe2020word2vec}, where many research questions have been posed, with a crucial point on \textit{"Can we answer queries on the embedded data?"}, instead of current approaches, that exploit embeddings to answer queries. The main point arisen is that there is a lack of a unified framework, or theory, that clearly indicates the path through embeddings complete integration. 


In this thesis, we focus on the problem of \textit{using embeddings for automatic augmentation of data to improve machine learning tasks}. We believe that it is a crucial integration task that has not yet been explored, either as an augmentation problem itself nor with the usage of embeddings. Few approaches that try to augment tables concerning a repository have been proposed; the two closest approaches proposed until now show different problems: (i) a set of rules to decide if it is safe to avoid a join or not \cite{kumar2016join}, restricted to a \textit{pure relational} settings, i.e., the schemas of each table is known; and (ii) a feature selection algorithm working on a matrix in which the number of attributes is much larger than the number of tuples \cite{chepurko2020arda}. 

The goal of this thesis is to study how to stabilize the automatic increase and make it scalable to possibly huge tables or data lakes. The idea is to define indexing structures, based on information-theoretic measures, to make the search for joining tables fast and adapt to the variety of existing joining possibilities (one-to-one, one-to-many, many-to-one, many-to-many). More precisely, the thesis will propose an index for horizontal augmentation on the data lakes scenario, as well as the integration of embeddings in the augmentation of tables via KBs. Eventually, an embedding algorithm for KBs will summarize in a unified framework the two previous results. Augmenting via KBs can be done with the usage of \textit{Knowledge Graphs}, exploiting the graph model to represent semantics information, e.g., links between entities, objects, or even abstract concepts. Knowledge Graphs (KGs) are usually associated with linked open data, and allow a user to perform queries directly on the graph and moving among different sources of open data, i.e., they allow for the retrieval of explicit knowledge. Furthermore, it is possible to add an ontology as a schema layer and exploit its logical rules inference system to retrieve also implicit knowledge.
We claim that a key ingredient to reaching automatic augmentation is the concept of functional dependency, thanks to its ability to catch (possibly fuzzy) relations between attributes in tables. Functional dependencies have been extended even to graphs \cite{fan2016functional}, so this concept seems suitable for both the scenarios we consider, i.e., tables augmentation and knowledge bases augmentation (as long as a KB can be seen as a graph). 
%We remand to Section \ref{fds} for further discussion on functional dependencies.


\bigbreak
\textbf{Previous Work.} From the above discussion, we would like to underline the three main topics of the proposal: \textit{open data}, \textit{data-driven augmentation} and \textit{embeddings}. To familiarize with these three main concepts, we deepened them in the first year and obtained some (published) results. Thanks to a long term collaboration with Prof. Michela Bertolotto (University College Dublin) and Dr. Laura Di Rocco (Northeastern University, Boston) started during my Master thesis entitled \textit{Embeddings for Geospatial Ontologies Representation}, we were able to embed geographical ontologies onto a suitable space, on which we first evaluated its quality \cite{dassereto2019evaluating} and then its impact as query engine into a data-driven microblogs geolocation algorithm \cite{di2020sherloc}. We further analyze the tuning of parameters to obtain higher quality embedding and summarized our results in \cite{dassereto2020tuning}; the key idea is that a fine-grained tuning of the parameters could drastically improve the quality of the embedding in capturing semantic similarities.

On the other hand, we also worked in the Open Data domain, trying to expand an existing approach on Linked Open Data source selection \cite{beyza2019linked} by introducing the concept of embedding to better associate context information. The work is in its conclusive part as the experimentation is going through and will be ready for submission soon. A discussion on the preliminary results obtained regarding the augmentation problem can be found in Section \ref{goals}.

The outline of this proposal is organized as follows: Section \ref{reference} places this document in a specific reference area and highlights the goals of the proposal; Section \ref{related} discusses the state of the art of the main topics of the proposal, i.e., open data augmentation, embeddings, and knowledge graphs; Section \ref{goals} presents in details the goals of the thesis, the methodology we plan to follow to reach them and the preliminary results obtained; finally, Section \ref{researchplan} shows the plan of work for the remaining two years. 
\bigbreak 
%{\color{red}Aggiungere qualcosa sulle dipendeze funzionali}


