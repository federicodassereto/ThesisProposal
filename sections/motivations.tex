\section{Motivation and Description}\label{motivations}

Since the rise of the World Wide Web, we have experienced an exponential growth in publicly available data. Data has become a crucial element in everyday life and many devices continuously produce and store them in a forest of different formats. This exponential growth has posed many challenges to scientists (even going so far as to create the figure of the \textit{data scientist}) in every step in which data are traditionally used: from data collection to integration, from processing to visualization. The impact of \textit{Big Data} has been summarized in \cite{mcafee2012big}, where the key properties are \textbf{V}olume, \textbf{V}elocity, \textbf{V}ariety, \textbf{V}eracity and \textbf{V}alue. 

In data science, it is increasingly the case that the main challenge is not in integrating known data, rather it is in finding the right data to solve a given data science problem. Today, data is a mass (uncountable) like dust, and data surrounds us like dust, even lovely structured data. Data is so cheap and easy to obtain that it is no longer important to always get the integration right and integrations are not static things (\textbf{V}elocity component). Data integration research has embraced and prospered by using approximation and machine learning. The uncontrolled nature of data manifests in large repositories of data (data lakes), in which both structured and unstructured data are stored. The peculiarity of data lakes lies in the fact that there is uncertainty about the presence of metadata describing the data themself. Furthermore, it is common the situation in which there is a lack of schemas, making traditional database approaches to integrating or querying data difficult to pursuit or even infeasible. Along with the uncertainty regarding the quality of the data, the amount of such data makes it infeasible also the traditional human-in-the-loop framework, since hand-labeling or manual rating of very large amounts of data is extremely expensive. It is easy to see that searching in data lakes is a complicated task, both in terms of time and methodology. 

Following the path traced by Tim Berners-Lee et al. in \cite{bizer2009linked} regarding Open Data publication and maintenance, many organizations are publishing data, augmentating tremendously the \textbf{V}olume and the \textbf{V}alue of such data. On the other hand, the explosion of sources providing data increases their \textbf{V}ariety, requiring new techniques to integrate data.
At a processing level, the main difficult arisen by the multiple sources and possibly unstructured data is given by the interpretation, i.e., the semantic layer. To extract semantics from unstructured data, in the last years the concept of embedding has been proposed. Embeddings are predominantly used in learning representations of texts \cite{mikolov2013efficient} and graphs \cite{nickel2017poincare}. In the last year, an approach exploting embedding to solve data integration task \cite{cappuzzo2020creating} has been published. This work can be seen as forerunner for the integration of embeddings in data management tasks, even if it works on tables from the same context.

A data scientist who is trying to solve machine learning tasks, frequently found herself in the situation in which there are not enough data to build a good model. Given the amount of data previously discussed, it would be very useful a framework which enables her to find automatically new features or samples to improve the model. Broadly, two main classes of augmentation can be distinguished: \textit{Horizontal Augmentation}, in which new significant features are added to the dataset and \textit{Vertical Augmentation}, in which new samples (tuples) are added to the dataset. Ideally, the two classes can be seen as the search for joinable (Horizontal) or unionable (Vertical) tables in the data lakes situation, and Knowledge Bases (KB) completion or extensions (vertical).

In this thesis, we focus on the problem of \textit{using embeddings for automatic augmentation of data to improve machine learning tasks}. We believe that it is a crucial integration task that has not yet been explored, either as augmentation problem itself nor with the usage of embeddings. Few approaches that try to augment tables with respect to a repository have been proposed; the two closest approaches proposed until now show different problems: (i) a set of rules to decide if it is safe to avoid a join or not \cite{kumar2016join}, restricted to a \textit{pure relational} settings, i.e., the schemas of each table is known; and (ii) a feature selection algorithm working on a matrix in which the number of attributes is much larger than the number of tuples \cite{chepurko2020arda}. 

The goal of this thesis is to study how to stabilize the automatic increase and make it scalable to possibly huge tables or data lakes. The idea is to define indexing structures, based on information theoretic measures, to make the search for joining tables fast and adapt to the variety of existing joining possibilities (one-to-one, one-to-many, many-to-one, many-to-many). More precisely, the thesis will proposed and index for horizontal augmentation on the data lakes scenario, as well as the integration of embeddings in the augmentation of KBs. Eventually, an embedding algorithm for KBs will summarize unified framework the two previous results.
\bigbreak
\textbf{Previous Works.} From the above discussion, we would like to underline the three main topics of the proposal: \textit{open data}, \textit{data-driven augmentation} and \textit{embeddings}. In order to familiarize with these three main concepts, we deepened them in the first year and published some works. Thanks to a long term collaboration with Prof. Michela Bertolotto (Univeristy College Dublin) and Laura Di Rocco (Northeastern University, Boston) started during my Master thesis entitled \textit{Embeddings for Geospatial Ontologies Representation}, we were able to embed geographical ontologies onto a suitable space, on which we first evaluated its quality \cite{dassereto2019evaluating} and then its impact as query engine into a data-driven microblogs geolocation algorithm \cite{di2020sherloc}. We further analyze the tuning of parameters to obtain higher quality embedding and summarized our results in \cite{dassereto2020tuning}; the key idea is that a fine-grained tuning of the parameters could drastically improve the quality of the embedding in capturing semantic similarities.

On the other hand, we also worked in the Open Data domain, trying to expand an existing approach on Linked Open Data source selection \cite{beyza2019linked} by introducing the concept of embedding to better associate context information. The work is in its conclusive part as the experimentation is going through and will be ready soon for submission. We remand to Section \ref{goals} for the discussion on the preliminary results obtained regarding the augmentation problem.



